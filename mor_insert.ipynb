{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark.sql.functions import sum as _sum, avg, expr, window, from_unixtime, col\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import time, datetime\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark application\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hudi:hudi-spark3.1-bundle_2.12:0.11.0,\"\n",
    "        \"org.apache.spark:spark-avro_2.12:3.1.1,\"\n",
    "        \"org.apache.hadoop:hadoop-aws:3.1.1,\"\n",
    "        \"com.amazonaws:aws-java-sdk:1.11.271,\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "    )\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1000\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"Asia/Ho_Chi_Minh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_timestamp(df, timestamp_col: str):\n",
    "    \"\"\"\n",
    "    parse timestamp following day, month, year\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame\n",
    "        timestamp_col (str): column of timestamp field\n",
    "        id_col (str): column id which is used to _id of hudi talbe\n",
    "    Returns:\n",
    "        DataFrame: return dataframe which adds day, month, year\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\"ts\", (col(f\"{timestamp_col}\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"year\", year(df.ts))\n",
    "        .withColumn(\"month\", month(df.ts))\n",
    "        .withColumn(\"day\", dayofmonth(expr(\"ts\")))\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ducdn/Desktop/workspace/hudi_mor/hudi_upsert'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hudi_table_name = \"hudi_upsert\"\n",
    "hudi_operation = \"UPSERT\"\n",
    "hudi_path = f\"/home/ducdn/Desktop/workspace/hudi_mor/{hudi_table_name}\"\n",
    "hudi_path\n",
    "# hudi_path = \"s3a://datalake/hudi_mor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hoodie_options = {\n",
    "    \"hoodie.table.name\": f\"{hudi_table_name}\",\n",
    "    \"hoodie.metadata.enable\": \"true\",\n",
    "    \"hoodie.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.operation\": hudi_operation,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"partition\",\n",
    "    \"hoodie.datasource.write.table.name\": f\"{hudi_table_name}\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"timestamp\",\n",
    "    \"hoodie.clean.automatic\": \"true\",\n",
    "    \"hoodie.cleaner.policy\": \"KEEP_LATEST_FILE_VERSIONS\",\n",
    "    \"hoodie.cleaner.fileversions.retained\": 8,\n",
    "    \"hoodie.compact.inline\": \"true\",\n",
    "    \"hoodie.compact.inline.max.delta.commits\": 3,\n",
    "     \"hoodie.datasource.hive_sync.support_timestamp\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"1\", \"value1\", 1687996799991),\n",
    "    (\"2\", \"value2\", 1687996799943),\n",
    "    (\"3\", \"value3\", 1687996799383),\n",
    "    (\"4\", \"value4\", 1687996799339),\n",
    "    (\"5\", \"value5\", 1687923298333),\n",
    "    (\"6\", \"value6\", 1687948498111),\n",
    "    (\"7\", \"value7\", 1687991698111),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+-----------------------+----+-----+---+\n",
      "|id |value |timestamp    |ts                     |year|month|day|\n",
      "+---+------+-------------+-----------------------+----+-----+---+\n",
      "|1  |value1|1687996799991|2023-06-29 06:59:59.991|2023|6    |29 |\n",
      "|2  |value2|1687996799943|2023-06-29 06:59:59.943|2023|6    |29 |\n",
      "|3  |value3|1687996799383|2023-06-29 06:59:59.383|2023|6    |29 |\n",
      "|4  |value4|1687996799339|2023-06-29 06:59:59.339|2023|6    |29 |\n",
      "|5  |value5|1687923298333|2023-06-28 10:34:58.333|2023|6    |28 |\n",
      "|6  |value6|1687948498111|2023-06-28 17:34:58.111|2023|6    |28 |\n",
      "|7  |value7|1687991698111|2023-06-29 05:34:58.111|2023|6    |29 |\n",
      "+---+------+-------------+-----------------------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = parsing_timestamp(df, \"timestamp\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/01 11:24:18 WARN HoodieBackedTableMetadata: Metadata table was not found at path /home/ducdn/Documents/workspace/hudi_mor/test/.hoodie/metadata\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"hudi\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .options(**hoodie_options) \\\n",
    "        .partitionBy(\"year,month, day\") \\\n",
    "        .save(\"/home/ducdn/Documents/workspace/hudi_mor/test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Hudi Mor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================START GEN DATA====================\n",
      "+---+-----+---------+---------+\n",
      "|id |value|timestamp|partition|\n",
      "+---+-----+---------+---------+\n",
      "+---+-----+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:40:39 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:39 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:40 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------STARTING WRITE HUDI TABLE--------------------\n",
      "--------------------DONE--------------------\n",
      "================== TIME SLEEP ==============\n",
      "====================START GEN DATA====================\n",
      "+------------------------------------+------+----------+---------+\n",
      "|id                                  |value |timestamp |partition|\n",
      "+------------------------------------+------+----------+---------+\n",
      "|4a090c34-4438-4bac-8b08-34d506009485|605253|1687232450|2        |\n",
      "+------------------------------------+------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:40:51 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:52 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:55 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------STARTING WRITE HUDI TABLE--------------------\n",
      "--------------------DONE--------------------\n",
      "================== TIME SLEEP ==============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m20\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDONE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m================== TIME SLEEP ==============\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m10\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def current_timestamp():\n",
    "    current_time = int(time.time())\n",
    "    return current_time\n",
    "\n",
    "def gen_data():\n",
    "    data = []\n",
    "    for i in range(0, 3):\n",
    "        i = uuid.uuid4()\n",
    "        value = random.randint(0, 1000000)\n",
    "        timestamp = current_timestamp()\n",
    "        partition = random.randint(1,3)\n",
    "        if partition != 2: continue\n",
    "        x = (str(i), value, timestamp, partition)\n",
    "        data.append(x)\n",
    "    return data\n",
    "\n",
    "while True:\n",
    "    print(\"=\"* 20 + \"START GEN DATA\" + \"=\"*20)\n",
    "    data = gen_data()\n",
    "    stream_df = spark.createDataFrame(data, schema)\n",
    "    # stream_df = stream_df.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "    stream_df.show(n=2, truncate=False)\n",
    "\n",
    "    stream_df.write.format(\"hudi\") \\\n",
    "        .options(**hoodie_options) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(hudi_path)\n",
    "\n",
    "    print(20*\"-\" + \"STARTING WRITE HUDI TABLE\" + \"-\"*20)\n",
    "\n",
    "    print(20*\"-\" + \"DONE\" + \"-\"*20)\n",
    "\n",
    "    print(f\"================== TIME SLEEP ==============\")\n",
    "    time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:35:31 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:35:32 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:35:34 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"fd6be044-0e71-4268-b9ae-8ac0a5sqwer\", 3306, current_timestamp(), 2)]\n",
    "stream_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "stream_df.write.format(\"hudi\") \\\n",
    "    .options(**hoodie_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(hudi_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

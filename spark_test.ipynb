{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 17:18:07 WARN Utils: Your hostname, ducdn resolves to a loopback address: 127.0.1.1; using 10.1.124.58 instead (on interface enp1s0)\n",
      "23/06/19 17:18:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ducdn/Documents/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ducdn/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ducdn/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.1-bundle_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eff842d3-18d6-4c1d-8a9e-7bac9613536c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hudi#hudi-spark3.1-bundle_2.12;0.13.0 in central\n",
      ":: resolution report :: resolve 199ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.hudi#hudi-spark3.1-bundle_2.12;0.13.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eff842d3-18d6-4c1d-8a9e-7bac9613536c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "23/06/19 17:18:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/19 17:18:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import sum as _sum, avg, expr, window, from_unixtime, col\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import time, datetime\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "spark = (\n",
    "        SparkSession.builder.appName(\"spark application\")\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "        )\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "        )\n",
    "        # .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        # .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        # .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://10.159.37.34:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "        )\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1000\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------+----------+---------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|                  id|     value| timestamp|partition|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------+----------+---------+\n",
      "|  20230619152433110|20230619152433110...|99d98e7b-d5c2-431...|                     2|b2cc23ea-1b60-44c...|99d98e7b-d5c2-431...|  11223344|1687163073|        2|\n",
      "|  20230619152117652|20230619152117652...|96234511-4154-48e...|                     2|b2cc23ea-1b60-44c...|96234511-4154-48e...| 123456789|1687162877|        2|\n",
      "|  20230619151758093|20230619151758093...|c017eb4f-a2e5-4bc...|                     2|b2cc23ea-1b60-44c...|c017eb4f-a2e5-4bc...|1687162678| 123456789|        2|\n",
      "|  20230619151342517|20230619151342517...|f5b9a1a3-d433-40c...|                     1|6e5cb8c6-ac87-46d...|f5b9a1a3-d433-40c...|    769703|1687162421|        1|\n",
      "|  20230619151342517|20230619151342517...|372a6869-0e30-456...|                     1|6e5cb8c6-ac87-46d...|372a6869-0e30-456...|    781405|1687162421|        1|\n",
      "|  20230619151359774|20230619151359774...|3760beb5-07c8-4b4...|                     1|6e5cb8c6-ac87-46d...|3760beb5-07c8-4b4...|    168215|1687162439|        1|\n",
      "|  20230619151359774|20230619151359774...|5df0f6d0-c220-46a...|                     1|6e5cb8c6-ac87-46d...|5df0f6d0-c220-46a...|    769303|1687162439|        1|\n",
      "|  20230619151411991|20230619151411991...|90b337ec-3b49-462...|                     1|6e5cb8c6-ac87-46d...|90b337ec-3b49-462...|    239788|1687162451|        1|\n",
      "|  20230619151342517|20230619151342517...|46ab01e0-1ef7-4d1...|                     3|702edb38-35d3-433...|46ab01e0-1ef7-4d1...|     99427|1687162421|        3|\n",
      "|  20230619151342517|20230619151342517...|428a7cb5-0bdf-42c...|                     3|702edb38-35d3-433...|428a7cb5-0bdf-42c...|    230094|1687162421|        3|\n",
      "|  20230619151342517|20230619151342517...|ff29aa40-b36b-462...|                     3|702edb38-35d3-433...|ff29aa40-b36b-462...|    543328|1687162421|        3|\n",
      "|  20230619151359774|20230619151359774...|690971fd-ef1e-4d8...|                     3|702edb38-35d3-433...|690971fd-ef1e-4d8...|    987145|1687162439|        3|\n",
      "|  20230619151411991|20230619151411991...|9548b8f4-e6dd-47f...|                     3|702edb38-35d3-433...|9548b8f4-e6dd-47f...|    537749|1687162451|        3|\n",
      "|  20230619151411991|20230619151411991...|6071f7ca-34fe-487...|                     3|702edb38-35d3-433...|6071f7ca-34fe-487...|    459335|1687162451|        3|\n",
      "|  20230619151411991|20230619151411991...|295f5628-023a-47c...|                     3|702edb38-35d3-433...|295f5628-023a-47c...|    235904|1687162451|        3|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------------------+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"hudi\").load(\"/home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mshow tables\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow(\u001b[39m100\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:110)\n\tat org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:103)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:88)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m hudiIncQueryDF \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread \\\n\u001b[1;32m      2\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hudi\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mhoodie.datasource.query.type\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mincremental\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mhoodie.datasource.read.begin.instanttime\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m20230619162114229\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m----> 5\u001b[0m     \u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39ms3a://datalake/hudi_mor\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m hudiIncQueryDF\u001b[39m.\u001b[39mcreateOrReplaceTempView(\u001b[39m\"\u001b[39m\u001b[39mhudi_mor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mselect * from hudi_mor\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:204\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    205\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o70.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:110)\n\tat org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:103)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:88)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:72)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "hudiIncQueryDF = spark.read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "    .option(\"hoodie.datasource.read.begin.instanttime\", \"20230619172801944\") \\\n",
    "    .load(\"s3a://datalake/hudi_mor\")\n",
    "\n",
    "hudiIncQueryDF.createOrReplaceTempView(\"hudi_mor\")\n",
    "spark.sql(\"select * from hudi_mor order by _hoodie_commit_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

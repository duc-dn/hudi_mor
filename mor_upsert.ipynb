{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import sum as _sum, avg, expr, window, from_unixtime, col\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import time, datetime\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark application\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hudi:hudi-spark3.1-bundle_2.12:0.13.0,\"\n",
    "        \"org.apache.spark:spark-avro_2.12:3.1.1,\"\n",
    "        \"org.apache.hadoop:hadoop-aws:3.1.1,\"\n",
    "        \"com.amazonaws:aws-java-sdk:1.11.271,\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "    )\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1000\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_table_name = \"hudi_bulk_insert\"\n",
    "hudi_operation = \"BULK_INSERT\"\n",
    "hudi_path = \"/home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\"\n",
    "# hudi_path = \"s3a://datalake/hudi_mor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hoodie_options = {\n",
    "    \"hoodie.table.name\": f\"{hudi_table_name}\",\n",
    "    \"hoodie.metadata.enable\": \"true\",\n",
    "    \"hoodie.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.operation\": hudi_operation,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"partition\",\n",
    "    \"hoodie.datasource.write.table.name\": f\"{hudi_table_name}\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"timestamp\",\n",
    "    \"hoodie.clean.automatic\": \"true\",\n",
    "    \"hoodie.cleaner.policy\": \"KEEP_LATEST_FILE_VERSIONS\",\n",
    "    \"hoodie.cleaner.fileversions.retained\": 8,\n",
    "    \"hoodie.compact.inline\": \"true\",\n",
    "    \"hoodie.compact.inline.max.delta.commits\": 3,\n",
    "    # \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    # \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    # \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    # \"hoodie.datasource.hive_sync.database\": \"default\",\n",
    "    # \"hoodie.datasource.hive_sync.table\": f\"hudi_mor\",\n",
    "    # \"hoodie.datasource.hive_sync.partition_fields\": \"partition\",\n",
    "    # \"hoodie.datasource.hive_sync.partition_extractor_class\": \n",
    "    #     \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "    # \"hoodie.datasource.hive_sync.metastore.uris\": \n",
    "    #     \"thrift://hive-metastore:9083\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"timestamp\", IntegerType()),\n",
    "    StructField(\"partition\", StringType())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Hudi Mor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================START GEN DATA====================\n",
      "+---+-----+---------+---------+\n",
      "|id |value|timestamp|partition|\n",
      "+---+-----+---------+---------+\n",
      "+---+-----+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:40:39 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:39 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:40 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------STARTING WRITE HUDI TABLE--------------------\n",
      "--------------------DONE--------------------\n",
      "================== TIME SLEEP ==============\n",
      "====================START GEN DATA====================\n",
      "+------------------------------------+------+----------+---------+\n",
      "|id                                  |value |timestamp |partition|\n",
      "+------------------------------------+------+----------+---------+\n",
      "|4a090c34-4438-4bac-8b08-34d506009485|605253|1687232450|2        |\n",
      "+------------------------------------+------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:40:51 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:52 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:40:55 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------STARTING WRITE HUDI TABLE--------------------\n",
      "--------------------DONE--------------------\n",
      "================== TIME SLEEP ==============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m20\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDONE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m================== TIME SLEEP ==============\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m10\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def current_timestamp():\n",
    "    current_time = int(time.time())\n",
    "    return current_time\n",
    "\n",
    "def gen_data():\n",
    "    data = []\n",
    "    for i in range(0, 3):\n",
    "        i = uuid.uuid4()\n",
    "        value = random.randint(0, 1000000)\n",
    "        timestamp = current_timestamp()\n",
    "        partition = random.randint(1,3)\n",
    "        if partition != 2: continue\n",
    "        x = (str(i), value, timestamp, partition)\n",
    "        data.append(x)\n",
    "    return data\n",
    "\n",
    "while True:\n",
    "    print(\"=\"* 20 + \"START GEN DATA\" + \"=\"*20)\n",
    "    data = gen_data()\n",
    "    stream_df = spark.createDataFrame(data, schema)\n",
    "    # stream_df = stream_df.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "    stream_df.show(n=2, truncate=False)\n",
    "\n",
    "    stream_df.write.format(\"hudi\") \\\n",
    "        .options(**hoodie_options) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(hudi_path)\n",
    "\n",
    "    print(20*\"-\" + \"STARTING WRITE HUDI TABLE\" + \"-\"*20)\n",
    "\n",
    "    print(20*\"-\" + \"DONE\" + \"-\"*20)\n",
    "\n",
    "    print(f\"================== TIME SLEEP ==============\")\n",
    "    time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 10:35:31 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:35:32 WARN HoodieWriteConfig: Embedded timeline server is disabled, fallback to use direct marker type for spark\n",
      "23/06/20 10:35:34 WARN BaseHoodieCompactionPlanGenerator: No operations are retrieved for /home/ducdn/Desktop/workspace/hudi_mor/hudi_mor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"fd6be044-0e71-4268-b9ae-8ac0a5sqwer\", 3306, current_timestamp(), 2)]\n",
    "stream_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "stream_df.write.format(\"hudi\") \\\n",
    "    .options(**hoodie_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(hudi_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
